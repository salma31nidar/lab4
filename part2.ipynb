{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8514894,"sourceType":"datasetVersion","datasetId":5083501},{"sourceId":8515145,"sourceType":"datasetVersion","datasetId":5083690}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nimport logging\nlogging.getLogger().setLevel(logging.CRITICAL)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndevice = 'cpu'\nif torch.cuda.is_available():\n    device = 'cuda'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-25T18:20:04.229452Z","iopub.execute_input":"2024-05-25T18:20:04.230102Z","iopub.status.idle":"2024-05-25T18:20:12.105035Z","shell.execute_reply.started":"2024-05-25T18:20:04.230049Z","shell.execute_reply":"2024-05-25T18:20:12.104158Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"device","metadata":{"execution":{"iopub.status.busy":"2024-05-25T18:20:14.304845Z","iopub.execute_input":"2024-05-25T18:20:14.305445Z","iopub.status.idle":"2024-05-25T18:20:14.312492Z","shell.execute_reply.started":"2024-05-25T18:20:14.305407Z","shell.execute_reply":"2024-05-25T18:20:14.311515Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\nmodel = GPT2LMHeadModel.from_pretrained('gpt2-medium')\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-25T18:20:14.871492Z","iopub.execute_input":"2024-05-25T18:20:14.872095Z","iopub.status.idle":"2024-05-25T18:20:44.047717Z","shell.execute_reply.started":"2024-05-25T18:20:14.872061Z","shell.execute_reply":"2024-05-25T18:20:44.046526Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45ff4d74f7d547f2aba6672bb3378af0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea02b07421f945a58d84430830431d94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c337e4a7a3314d4cb7afa633a559a87e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75e975ccf52e4acdbfe59e5419300b67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f1528331c5c4865b52ff48e3198ac98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d853381b105047f9b27e14cadbf6b4a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4283dacb752468b87960ea1f3c64afa"}},"metadata":{}}]},{"cell_type":"code","source":"def choose_from_top(probs, n=5):\n    ind = np.argpartition(probs, -n)[-n:]\n    top_prob = probs[ind]\n    top_prob = top_prob / np.sum(top_prob) # Normalize\n    choice = np.random.choice(n, 1, p = top_prob)\n    token_id = ind[choice][0]\n    return int(token_id)","metadata":{"execution":{"iopub.status.busy":"2024-05-25T18:20:44.049429Z","iopub.execute_input":"2024-05-25T18:20:44.049744Z","iopub.status.idle":"2024-05-25T18:20:44.055491Z","shell.execute_reply.started":"2024-05-25T18:20:44.049715Z","shell.execute_reply":"2024-05-25T18:20:44.054503Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nimport json\nimport csv\n\nclass YelpDataset(Dataset):\n    def __init__(self):\n        super().__init__()\n\n        self.food_list = []\n        self.end_of_text_token = \"<|endoftext|>\"\n\n        with open(\"/kaggle/input/yelp-for-sentiment/yelp.csv\") as csv_file:\n            csv_reader = csv.reader(csv_file, delimiter=',')\n\n            x = 0\n            for row in csv_reader:\n                joke_str = f\"recepies:{row[0]}{self.end_of_text_token}\"\n                self.food_list.append(joke_str)\n\n    def __len__(self):\n       return len(self.food_list)\n\n    def __getitem__(self, item):\n        return self.food_list[item]","metadata":{"execution":{"iopub.status.busy":"2024-05-25T18:20:44.056883Z","iopub.execute_input":"2024-05-25T18:20:44.057594Z","iopub.status.idle":"2024-05-25T18:20:44.068008Z","shell.execute_reply.started":"2024-05-25T18:20:44.057557Z","shell.execute_reply":"2024-05-25T18:20:44.067074Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"dataset = YelpDataset()\nyelp_loader = DataLoader(dataset, batch_size=1, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-25T18:20:44.069872Z","iopub.execute_input":"2024-05-25T18:20:44.070154Z","iopub.status.idle":"2024-05-25T18:20:44.224505Z","shell.execute_reply.started":"2024-05-25T18:20:44.070129Z","shell.execute_reply":"2024-05-25T18:20:44.223580Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 4\nEPOCHS = 7\nLEARNING_RATE = 3e-5\nWARMUP_STEPS = 5000\nMAX_SEQ_LEN = 800\nfrom transformers import AdamW, get_linear_schedule_with_warmup\ndevice = 'cpu'\nif torch.cuda.is_available():\n    device = 'cuda'","metadata":{"execution":{"iopub.status.busy":"2024-05-25T18:20:44.225702Z","iopub.execute_input":"2024-05-25T18:20:44.225998Z","iopub.status.idle":"2024-05-25T18:20:44.241076Z","shell.execute_reply.started":"2024-05-25T18:20:44.225973Z","shell.execute_reply":"2024-05-25T18:20:44.240256Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"device","metadata":{"execution":{"iopub.status.busy":"2024-05-25T18:20:44.242277Z","iopub.execute_input":"2024-05-25T18:20:44.242635Z","iopub.status.idle":"2024-05-25T18:20:44.248777Z","shell.execute_reply.started":"2024-05-25T18:20:44.242595Z","shell.execute_reply":"2024-05-25T18:20:44.247580Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"model = model.to(device)\nmodel.train()\noptimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps = -1)\nproc_seq_count = 0\nsum_loss = 0.0\nbatch_count = 0\n\ntmp_yelp_tens = None\nmodels_folder = \"trained_models\"\nif not os.path.exists(models_folder):\n    os.mkdir(models_folder)\n\nfor epoch in range(EPOCHS):\n    \n    print(f\"EPOCH {epoch} started\" + '=' * 30)\n    \n    for idx,yelp in enumerate(yelp_loader):\n        \n        #################### \"Fit as many recipes sequences into MAX_SEQ_LEN sequence as possible\" logic start ####\n        yelp_tens = torch.tensor(tokenizer.encode(yelp[0])).unsqueeze(0).to(device)\n        #Skip sample from dataset if it is longer than MAX_SEQ_LEN\n        if yelp_tens.size()[1] > MAX_SEQ_LEN:\n            continue\n        \n        #The first recipes sequence in the sequence\n        if not torch.is_tensor(tmp_yelp_tens):\n            tmp_yelp_tens = yelp_tens\n            continue\n        else:\n            #The next recipes does not fit in so we process the sequence and leave the last recipes \n            #as the start for next sequence \n            if tmp_yelp_tens.size()[1] + yelp_tens.size()[1] > MAX_SEQ_LEN:\n                work_yelp_tens = tmp_yelp_tens\n                tmp_yelp_tens = yelp_tens\n            else:\n                #Add the recipes to sequence, continue and try to add more\n                tmp_yelp_tens = torch.cat([tmp_yelp_tens, yelp_tens[:,1:]], dim=1)\n                continue\n        ################## Sequence ready, process it trough the model ##################\n            \n        outputs = model(work_yelp_tens, labels=work_yelp_tens)\n        loss, logits = outputs[:2]                        \n        loss.backward()\n        sum_loss = sum_loss + loss.detach().data\n                       \n        proc_seq_count = proc_seq_count + 1\n        if proc_seq_count == BATCH_SIZE:\n            proc_seq_count = 0    \n            batch_count += 1\n            optimizer.step()\n            scheduler.step() \n            optimizer.zero_grad()\n            model.zero_grad()\n\n        if batch_count == 100:\n            print(f\"sum loss {sum_loss}\")\n            batch_count = 0\n            sum_loss = 0.0\n    \n    # Store the model after each epoch to compare the performance of them\ntorch.save(model.state_dict(), os.path.join(models_folder, f\"gpt2_medium_food_{epoch}.pt\"))","metadata":{"execution":{"iopub.status.busy":"2024-05-25T18:20:44.250398Z","iopub.execute_input":"2024-05-25T18:20:44.250729Z","iopub.status.idle":"2024-05-25T18:33:46.376707Z","shell.execute_reply.started":"2024-05-25T18:20:44.250696Z","shell.execute_reply":"2024-05-25T18:33:46.375847Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"EPOCH 0 started==============================\nEPOCH 1 started==============================\nsum loss 2048.149658203125\nEPOCH 2 started==============================\nsum loss 1827.6429443359375\nEPOCH 3 started==============================\nEPOCH 4 started==============================\nsum loss 1697.4271240234375\nEPOCH 5 started==============================\nsum loss 1672.11474609375\nEPOCH 6 started==============================\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\n\ndef choose_from_top(probs, n=5):\n    ind = np.argpartition(probs, -n)[-n:]\n    top_probs = probs[ind]\n    top_probs = top_probs / np.sum(top_probs)  # Normalize\n    chosen_index = np.random.choice(ind, 1, p=top_probs)\n    return chosen_index[0]\n\nMODEL_EPOCH = 6\nmodels_folder = \"trained_models\"\nmodel_path = os.path.join(models_folder, f\"gpt2_medium_food_{MODEL_EPOCH}.pt\")\n\n# Ensure the device is properly set\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the model and tokenizer\nmodel = GPT2LMHeadModel.from_pretrained('gpt2-medium')\nmodel.load_state_dict(torch.load(model_path, map_location=device))\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n\nmodel.to(device)\nmodel.eval()\n\nyelp_output_file_path = f'generated_{MODEL_EPOCH}.yelp'\n\nif os.path.exists(yelp_output_file_path):\n    os.remove(yelp_output_file_path)\n\nyelp_num = 0\nwith torch.no_grad():\n    for yelp_idx in range(5):\n        yelp_finished = False\n        cur_ids = torch.tensor(tokenizer.encode(\"yelp review: \")).unsqueeze(0).to(device)\n\n        for i in range(100):\n            outputs = model(cur_ids)\n            logits = outputs.logits\n            softmax_logits = torch.softmax(logits[0, -1], dim=0)  # Take the first batch and the last predicted embedding\n\n            if i < 3:\n                n = 20\n            else:\n                n = 3\n\n            next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=n)  # Randomly select the next word\n            cur_ids = torch.cat([cur_ids, torch.ones((1, 1)).long().to(device) * next_token_id], dim=1)  # Add the last word to the running sequence\n\n            # Token to check for stopping condition (adjust this as needed)\n            if next_token_id == tokenizer.eos_token_id:\n                yelp_finished = True\n                break\n\n        output_list = list(cur_ids.squeeze().to('cpu').numpy())\n        output_text = tokenizer.decode(output_list)\n\n        # Print the generated text\n        print(f\"Generated Text {yelp_num + 1}: {output_text}\")\n\n        with open(yelp_output_file_path, 'a') as f:\n            f.write(f\"{output_text} \\n\\n\")\n\n        # Print confirmation of saving the file\n        print(f\"Text {yelp_num + 1} saved to {yelp_output_file_path}\")\n\n        yelp_num += 1\n","metadata":{"execution":{"iopub.status.busy":"2024-05-25T18:46:40.249399Z","iopub.execute_input":"2024-05-25T18:46:40.250594Z","iopub.status.idle":"2024-05-25T18:46:46.569144Z","shell.execute_reply.started":"2024-05-25T18:46:40.250556Z","shell.execute_reply":"2024-05-25T18:46:46.568221Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Generated Text 1: yelp review:  @shaunmccarthy\nPosted by Shaun McCarthy at 12:00 am<|endoftext|>\nText 1 saved to generated_6.yelp\nGenerated Text 2: yelp review:  It's a good book, and it's not a bad book. I think it could have used a little more of a twist or two to keep things interesting, and it could have been a little more fun. I think it would have been a lot more fun to see the book's ending.<|endoftext|>\nText 2 saved to generated_6.yelp\nGenerated Text 3: yelp review:  a great read.\nPosted by  the_lady at 11:34 AM<|endoftext|>\nText 3 saved to generated_6.yelp\nGenerated Text 4: yelp review:  \"I've been a fan of the book since it was published, so when it came out, I was excited to see what it would do. It's a fun read, and I'm glad I read it.\"<|endoftext|>\nText 4 saved to generated_6.yelp\nGenerated Text 5: yelp review:  http://www.youtube.com/watch?v=_0QQQQ-0__Q4&feature=youtu.be\nPosted by  Lilith at 12:03 PM<|endoftext|>\nText 5 saved to generated_6.yelp\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}